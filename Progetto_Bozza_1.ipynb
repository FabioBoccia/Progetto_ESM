{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabioBoccia/Progetto_ESM/blob/dev/Progetto_Bozza_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j81hHyT9cB4t"
      },
      "outputs": [],
      "source": [
        "%rm -r sample_data\n",
        "!wget --user=corso --password=p2021corso http://www.grip.unina.it/download/corso/ffhq_real.zip\n",
        "!unzip -q ffhq_real.zip\n",
        "!mkdir -p ffhq/train/real\n",
        "!mkdir -p ffhq/train/synthesized\n",
        "!mkdir -p ffhq/test/real\n",
        "!mkdir -p ffhq/test/synthesized\n",
        "%cd /content/0_real/\n",
        "!mv $(ls | shuf -n 2500) ../ffhq/train/real/\n",
        "!mv * ../ffhq/test/real/\n",
        "%cd ..\n",
        "!rm -d 0_real\n",
        "\n",
        "!pip install tensorflow_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQIZZWBlk2yE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/samples/top_k_300_top_p_1.0/\n",
        "!mkdir -p /content/tmp/\n",
        "!cp * /content/tmp/\n",
        "%cd /content/tmp/\n",
        "!mv $(ls | shuf -n 2500) /content/ffhq/train/synthesized/\n",
        "!mv * /content/ffhq/test/synthesized/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAs3Yto65dAc"
      },
      "outputs": [],
      "source": [
        "!%reset -f\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TbVW9iEdHHr"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "interpolation = 'bilinear'\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    'ffhq/train/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(256, 256),\n",
        "    shuffle=True,\n",
        "    seed=125,\n",
        "    validation_split=1/5, # Validation is 1/6 of the total, so 1/5 of (total - test)\n",
        "    subset='training',\n",
        "    interpolation=interpolation,\n",
        "    crop_to_aspect_ratio=True,\n",
        ")\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    'ffhq/train/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(256, 256),\n",
        "    shuffle=True,\n",
        "    seed=125,\n",
        "    validation_split=1/5,\n",
        "    subset='validation',\n",
        "    interpolation=interpolation,\n",
        "    crop_to_aspect_ratio=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uuqVA6upVE5"
      },
      "outputs": [],
      "source": [
        "k = tf.random.uniform(\n",
        "        shape=[],\n",
        "        minval=0.0,\n",
        "        maxval=3.0,\n",
        "        dtype=tf.dtypes.float32\n",
        "    )\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1bUYLvMj2pE"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "d = 3\n",
        "def randomGaussianBlur(x):\n",
        "    k = tf.random.uniform(\n",
        "        shape=[],\n",
        "        minval=0.0,\n",
        "        maxval=3.0,\n",
        "        dtype=tf.float32\n",
        "    )\n",
        "    return tfio.experimental.filter.gaussian(x, d, k)\n",
        "\n",
        "def randomRot90(x):\n",
        "    k = tf.random.uniform(\n",
        "        shape=[],\n",
        "        minval=0,\n",
        "        maxval=4,\n",
        "        dtype=tf.dtypes.int32,\n",
        "        seed=None,\n",
        "        name=None\n",
        "    )\n",
        "    return tf.image.rot90(x, k)\n",
        "\n",
        "def preprocess(x):\n",
        "    y = tf.keras.layers.CenterCrop(224,224)(x)\n",
        "    y = tf.keras.layers.Rescaling(1./127.5, offset=-1)(x)\n",
        "    return y\n",
        "\n",
        "def augment(x):\n",
        "    y = randomGaussianBlur(x)\n",
        "    y = randomRot90(x)\n",
        "    return y\n",
        "\n",
        "val_dataset = val_dataset.map(lambda x, y: (preprocess(x), y))\n",
        "train_dataset = train_dataset.map(lambda x, y: (augment(preprocess(x)), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjSEiSr1rkMa"
      },
      "outputs": [],
      "source": [
        "# Reference: https://www.tensorflow.org/tutorials/load_data/images#configure_the_dataset_for_performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deb1ezHKrzE4"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.Xception(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(256,256,3),\n",
        "    pooling=None, # hyperparameter?\n",
        ")\n",
        "#train_dataset = tf.keras.applications.xception.preprocess_input(train_dataset)\n",
        "#val_dataset = tf.keras.applications.xception.preprocess_input(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oACkh7F-wkGe"
      },
      "outputs": [],
      "source": [
        "# se presente lr scheduler, il fine tunining del lr va fatto lì\n",
        "opt = tf.keras.optimizers.Nadam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        ")\n",
        "loss = tf.keras.losses.BinaryCrossentropy(\n",
        "    from_logits=False,\n",
        "    label_smoothing=0.0,\n",
        "    axis=-1,\n",
        "    reduction=tf.keras.losses.Reduction.AUTO,\n",
        ")\n",
        "metric = tf.keras.metrics.AUC(\n",
        "    num_thresholds=200,\n",
        "    curve='ROC',\n",
        "    summation_method='interpolation',\n",
        "    name=None,\n",
        "    dtype=None,\n",
        "    thresholds=None,\n",
        "    multi_label=False,\n",
        "    num_labels=None,\n",
        "    label_weights=None,\n",
        "    from_logits=False # ??\n",
        ")\n",
        "\n",
        "train_after_layer = 100\n",
        "for layer in base_model.layers[:train_after_layer]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(tf.keras.layers.GlobalMaxPooling2D())\n",
        "model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=loss,\n",
        "    metrics=metric\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LEARNING RATE SCHEDULING - EXPONENTIAL DECAY\n",
        "# lr0 - learning rate iniziale\n",
        "# s - numero di step\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 ** (epoch / s)\n",
        "    return exponential_decay_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ4UgBcCx4px"
      },
      "outputs": [],
      "source": [
        "batch_size=1000\n",
        "epochs = 2\n",
        "for i in range(51):\n",
        "\n",
        "    # lr *0,1 dopo metà numero totale delle epoche\n",
        "    exponential_decay_fn = exponential_decay(0.01, s=(1+i))\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        epochs=(1+i)*epochs,\n",
        "        initial_epoch=i*epochs,\n",
        "        verbose=\"auto\",\n",
        "        #validation_data=val_dataset,\n",
        "        #validation_freq=1,\n",
        "        shuffle=True,\n",
        "        callbacks=[lr_scheduler] # lr scheduling\n",
        "    )\n",
        "    loss, AUC = model.evaluate(val_dataset)\n",
        "    model.save_weights('drive/MyDrive/pesi_ffhq/%d_%.3f_%.3f.h5' %((i+1)*epochs, loss, AUC))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    'ffhq/test/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    batch_size=None,\n",
        "    image_size=(256, 256),\n",
        "    shuffle=True,\n",
        "    seed=125,\n",
        "    validation_split=0,\n",
        "    interpolation=interpolation,\n",
        "    crop_to_aspect_ratio=True,\n",
        ")\n",
        "\n",
        "loss, AUC = model.evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Copy of Copy of Progetto_Bozza_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
