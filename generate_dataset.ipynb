{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4hdc6YonrvoC",
        "5gaAQZXTxFxD"
      ],
      "mount_file_id": "1rChQolncIbyhfMnd6QIVDKbDNFGP2tFA",
      "authorship_tag": "ABX9TyNp6MzvumSRTxnrTPeTjDz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabioBoccia/Progetto_ESM/blob/dev/generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U0NA9HrrZey"
      },
      "source": [
        "# Taming Transformers\n",
        "\n",
        "This notebook is a minimal working example to generate landscape images as in [Taming Transformers for High-Resolution Image Synthesis](https://github.com/CompVis/taming-transformers). **tl;dr** We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hdc6YonrvoC"
      },
      "source": [
        "## Setup\n",
        "The setup code in this section was written to be [run in a Colab environment](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb). For a full, local setup, we recommend the provided [conda environment](https://github.com/CompVis/taming-transformers/blob/master/environment.yaml), as [described in the readme](https://github.com/CompVis/taming-transformers#requirements). This will also allow you to run a streamlit based demo.\n",
        "\n",
        "Here, we first clone the repository and download a model checkpoint and config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwj8j_l201aF"
      },
      "source": [
        "%cd /content\n",
        "%rm -r sample_data\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "%cd taming-transformers\n",
        "!mkdir -p logs\n",
        "!cp -r /content/drive/MyDrive/2021-04-23T18-19-01_ffhq_transformer logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeBqWgMQDjZb"
      },
      "source": [
        "Next, we install minimal required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzQNmIuT_0uF"
      },
      "source": [
        "!pip install --upgrade omegaconf einops transformers pytorch-lightning\n",
        "#!pip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gaAQZXTxFxD"
      },
      "source": [
        "## Loading the model\n",
        "\n",
        "We load and print the config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUOUJaTj02Bq"
      },
      "source": [
        "import sys\n",
        "%cd /content/taming-transformers\n",
        "sys.path.append(\".\")\n",
        "from omegaconf import OmegaConf\n",
        "config_path = \"/content/taming-transformers/logs/2021-04-23T18-19-01_ffhq_transformer/configs/2021-04-23T18-19-01-project.yaml\"\n",
        "config = OmegaConf.load(config_path)\n",
        "import yaml\n",
        "print(yaml.dump(OmegaConf.to_container(config)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzQqNgiLEJ9J"
      },
      "source": [
        "Instantiate the model and load the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWvDDVoz3RB4"
      },
      "source": [
        "from taming.models.cond_transformer import Net2NetTransformer\n",
        "import torch\n",
        "model = Net2NetTransformer(**config.model.params)\n",
        "ckpt_path = \"/content/taming-transformers/logs/2021-04-23T18-19-01_ffhq_transformer/checkpoints/last.ckpt\"\n",
        "sd = torch.load(ckpt_path, map_location=\"cuda\")[\"state_dict\"]\n",
        "missing, unexpected = model.load_state_dict(sd, strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYN9cREY-r3N"
      },
      "source": [
        "model.cuda().eval()\n",
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftDWneY3zJZ5"
      },
      "source": [
        "## Sample an image\n",
        "\n",
        "```\n",
        "# Default values:\n",
        "num_samples = 3000\n",
        "batch_size = 3\n",
        "top_k = 300\n",
        "top_p = 0.85\n",
        "temperature = 1.0\n",
        "\n",
        "logdir=\"/content/drive/MyDrive/\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 3000\n",
        "batch_size = 3\n",
        "top_k = 300\n",
        "top_p = 0.85\n",
        "temperature = 1.0\n",
        "\n",
        "logdir=\"/content/drive/MyDrive/\""
      ],
      "metadata": {
        "id": "-CjPHdSRv_kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.sample_fast import sample_unconditional\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def show_image(s):\n",
        "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0]\n",
        "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
        "  s = Image.fromarray(s)\n",
        "  display(s)\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_live(model, batch_size, temperature, top_k, unconditional=True, num_samples=50000, top_p=None):\n",
        "    print(f\"Running in unconditional sampling mode, producing {num_samples} samples.\")\n",
        "    batches = [batch_size for _ in range(num_samples//batch_size)] + [num_samples % batch_size]\n",
        "    for n, bs in tqdm(enumerate(batches), desc=\"Sampling\"):\n",
        "        if bs == 0: break\n",
        "        logs = sample_unconditional(model, batch_size=bs, temperature=temperature, top_k=top_k, top_p=top_p)\n",
        "        show_image(logs[\"samples\"])"
      ],
      "metadata": {
        "id": "Jc36JX3yzDZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.sample_fast import run\n",
        "import os \n",
        "\n",
        "logdir = os.path.join(logdir, \"samples\", f\"top_k_{top_k}_temp_{temperature:.2f}_top_p_{top_p}\")\n",
        "\n",
        "print(f\"Logging to {logdir}\")\n",
        "os.makedirs(logdir, exist_ok=True)\n",
        "\n",
        "run_live(model, batch_size, temperature, top_k, unconditional=model.be_unconditional, num_samples=num_samples, top_p=top_p)\n",
        "#run(logdir, model, batch_size, temperature, top_k, unconditional=model.be_unconditional, num_samples=num_samples, top_p=top_p)"
      ],
      "metadata": {
        "id": "XnF32AnQxnyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}